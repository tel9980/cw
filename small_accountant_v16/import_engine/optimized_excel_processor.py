#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¼˜åŒ–çš„Excelå¤„ç†å™¨ - æå‡å¤§æ–‡ä»¶å¤„ç†æ€§èƒ½

Feature: small-accountant-practical-enhancement
Optimization: Excel processing performance
"""

import pandas as pd
import numpy as np
from typing import Iterator, List, Dict, Any, Optional, Tuple
from pathlib import Path
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
import logging
from dataclasses import dataclass
from decimal import Decimal
from datetime import datetime

from ..models.core_models import TransactionRecord, TransactionType, TransactionStatus
from ..core.exceptions import ImportError, ValidationError


@dataclass
class ProcessingStats:
    """å¤„ç†ç»Ÿè®¡ä¿¡æ¯"""
    total_rows: int = 0
    processed_rows: int = 0
    error_rows: int = 0
    processing_time: float = 0.0
    memory_usage_mb: float = 0.0
    
    @property
    def success_rate(self) -> float:
        """æˆåŠŸç‡"""
        if self.total_rows == 0:
            return 0.0
        return (self.processed_rows / self.total_rows) * 100


class ProgressCallback:
    """è¿›åº¦å›è°ƒæ¥å£"""
    
    def __init__(self, total_steps: int):
        self.total_steps = total_steps
        self.current_step = 0
        self.start_time = time.time()
    
    def update(self, step: int, message: str = ""):
        """æ›´æ–°è¿›åº¦"""
        self.current_step = step
        progress = (step / self.total_steps) * 100
        elapsed = time.time() - self.start_time
        
        if step > 0:
            eta = (elapsed / step) * (self.total_steps - step)
            print(f"\rè¿›åº¦: {progress:.1f}% ({step}/{self.total_steps}) - {message} - é¢„è®¡å‰©ä½™: {eta:.1f}ç§’", end="")
        else:
            print(f"\rè¿›åº¦: {progress:.1f}% - {message}", end="")
    
    def finish(self):
        """å®Œæˆè¿›åº¦"""
        elapsed = time.time() - self.start_time
        print(f"\nâœ… å¤„ç†å®Œæˆï¼Œæ€»è€—æ—¶: {elapsed:.2f}ç§’")


class OptimizedExcelProcessor:
    """ä¼˜åŒ–çš„Excelå¤„ç†å™¨
    
    ç‰¹æ€§ï¼š
    - åˆ†å—è¯»å–å¤§æ–‡ä»¶
    - å¤šçº¿ç¨‹å¹¶è¡Œå¤„ç†
    - å†…å­˜ä½¿ç”¨ä¼˜åŒ–
    - å®æ—¶è¿›åº¦æ˜¾ç¤º
    - é”™è¯¯æ¢å¤æœºåˆ¶
    """
    
    def __init__(self, 
                 chunk_size: int = 1000,
                 max_workers: int = 4,
                 memory_limit_mb: int = 500):
        """
        åˆå§‹åŒ–ä¼˜åŒ–å¤„ç†å™¨
        
        Args:
            chunk_size: æ¯æ¬¡å¤„ç†çš„è¡Œæ•°
            max_workers: æœ€å¤§å·¥ä½œçº¿ç¨‹æ•°
            memory_limit_mb: å†…å­˜ä½¿ç”¨é™åˆ¶(MB)
        """
        self.chunk_size = chunk_size
        self.max_workers = max_workers
        self.memory_limit_mb = memory_limit_mb
        self.logger = logging.getLogger(__name__)
        
        # æ€§èƒ½ç»Ÿè®¡
        self.stats = ProcessingStats()
        
        # ç¼“å­˜ä¼˜åŒ–
        self._column_mapping_cache = {}
        self._validation_cache = {}
    
    def process_excel_file(self, 
                          file_path: Path,
                          column_mapping: Dict[str, str],
                          progress_callback: Optional[ProgressCallback] = None) -> Tuple[List[TransactionRecord], ProcessingStats]:
        """
        ä¼˜åŒ–çš„Excelæ–‡ä»¶å¤„ç†
        
        Args:
            file_path: Excelæ–‡ä»¶è·¯å¾„
            column_mapping: åˆ—æ˜ å°„é…ç½®
            progress_callback: è¿›åº¦å›è°ƒ
            
        Returns:
            å¤„ç†ç»“æœå’Œç»Ÿè®¡ä¿¡æ¯
        """
        start_time = time.time()
        
        try:
            # 1. é¢„å¤„ç†ï¼šè·å–æ–‡ä»¶ä¿¡æ¯
            file_info = self._analyze_file(file_path)
            self.stats.total_rows = file_info['total_rows']
            
            if progress_callback:
                progress_callback.total_steps = self.stats.total_rows
                progress_callback.update(0, "å¼€å§‹å¤„ç†Excelæ–‡ä»¶...")
            
            # 2. åˆ†å—è¯»å–å’Œå¤„ç†
            all_records = []
            
            if file_info['total_rows'] <= self.chunk_size:
                # å°æ–‡ä»¶ï¼šç›´æ¥å¤„ç†
                records = self._process_small_file(file_path, column_mapping, progress_callback)
                all_records.extend(records)
            else:
                # å¤§æ–‡ä»¶ï¼šåˆ†å—å¤„ç†
                records = self._process_large_file(file_path, column_mapping, progress_callback)
                all_records.extend(records)
            
            # 3. åå¤„ç†ï¼šæ•°æ®æ¸…ç†å’ŒéªŒè¯
            if progress_callback:
                progress_callback.update(self.stats.total_rows, "æ•°æ®éªŒè¯ä¸­...")
            
            validated_records = self._post_process_records(all_records)
            
            # 4. ç»Ÿè®¡ä¿¡æ¯
            self.stats.processed_rows = len(validated_records)
            self.stats.processing_time = time.time() - start_time
            self.stats.memory_usage_mb = self._get_memory_usage()
            
            if progress_callback:
                progress_callback.finish()
            
            self.logger.info(f"Excelå¤„ç†å®Œæˆ: {self.stats.processed_rows}/{self.stats.total_rows} è¡Œï¼Œ"
                           f"æˆåŠŸç‡: {self.stats.success_rate:.1f}%ï¼Œè€—æ—¶: {self.stats.processing_time:.2f}ç§’")
            
            return validated_records, self.stats
            
        except Exception as e:
            self.logger.error(f"Excelå¤„ç†å¤±è´¥: {e}")
            raise ImportError(f"Excelæ–‡ä»¶å¤„ç†å¤±è´¥: {e}")
    
    def _analyze_file(self, file_path: Path) -> Dict[str, Any]:
        """åˆ†ææ–‡ä»¶åŸºæœ¬ä¿¡æ¯"""
        try:
            # å¿«é€Ÿè¯»å–æ–‡ä»¶ä¿¡æ¯
            with pd.ExcelFile(file_path) as xls:
                sheet_names = xls.sheet_names
                
                # è¯»å–ç¬¬ä¸€ä¸ªå·¥ä½œè¡¨çš„åŸºæœ¬ä¿¡æ¯
                df_info = pd.read_excel(xls, sheet_name=0, nrows=0)
                columns = df_info.columns.tolist()
                
                # è·å–æ€»è¡Œæ•°ï¼ˆå¿«é€Ÿæ–¹æ³•ï¼‰
                df_sample = pd.read_excel(xls, sheet_name=0)
                total_rows = len(df_sample)
                
                return {
                    'total_rows': total_rows,
                    'columns': columns,
                    'sheet_names': sheet_names,
                    'file_size_mb': file_path.stat().st_size / (1024 * 1024)
                }
        except Exception as e:
            raise ImportError(f"æ— æ³•åˆ†æExcelæ–‡ä»¶: {e}")
    
    def _process_small_file(self, 
                           file_path: Path, 
                           column_mapping: Dict[str, str],
                           progress_callback: Optional[ProgressCallback] = None) -> List[TransactionRecord]:
        """å¤„ç†å°æ–‡ä»¶ï¼ˆå•çº¿ç¨‹ï¼‰"""
        try:
            df = pd.read_excel(file_path)
            records = []
            
            for idx, row in df.iterrows():
                try:
                    record = self._convert_row_to_record(row, column_mapping)
                    if record:
                        records.append(record)
                        self.stats.processed_rows += 1
                    
                    if progress_callback and idx % 100 == 0:
                        progress_callback.update(idx + 1, f"å¤„ç†ç¬¬ {idx + 1} è¡Œ")
                        
                except Exception as e:
                    self.stats.error_rows += 1
                    self.logger.warning(f"ç¬¬ {idx + 1} è¡Œå¤„ç†å¤±è´¥: {e}")
            
            return records
            
        except Exception as e:
            raise ImportError(f"å°æ–‡ä»¶å¤„ç†å¤±è´¥: {e}")
    
    def _process_large_file(self, 
                           file_path: Path, 
                           column_mapping: Dict[str, str],
                           progress_callback: Optional[ProgressCallback] = None) -> List[TransactionRecord]:
        """å¤„ç†å¤§æ–‡ä»¶ï¼ˆåˆ†å—+å¤šçº¿ç¨‹ï¼‰"""
        try:
            all_records = []
            processed_rows = 0
            
            # åˆ†å—è¯»å–
            chunk_reader = pd.read_excel(file_path, chunksize=self.chunk_size)
            
            with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
                # æäº¤æ‰€æœ‰å—çš„å¤„ç†ä»»åŠ¡
                future_to_chunk = {}
                chunk_index = 0
                
                for chunk in chunk_reader:
                    future = executor.submit(self._process_chunk, chunk, column_mapping, chunk_index)
                    future_to_chunk[future] = chunk_index
                    chunk_index += 1
                
                # æ”¶é›†ç»“æœ
                for future in as_completed(future_to_chunk):
                    chunk_idx = future_to_chunk[future]
                    try:
                        chunk_records = future.result()
                        all_records.extend(chunk_records)
                        processed_rows += len(chunk_records)
                        
                        if progress_callback:
                            progress_callback.update(processed_rows, f"å·²å¤„ç† {processed_rows} è¡Œ")
                            
                    except Exception as e:
                        self.logger.error(f"å— {chunk_idx} å¤„ç†å¤±è´¥: {e}")
                        self.stats.error_rows += self.chunk_size
            
            return all_records
            
        except Exception as e:
            raise ImportError(f"å¤§æ–‡ä»¶å¤„ç†å¤±è´¥: {e}")
    
    def _process_chunk(self, 
                      chunk: pd.DataFrame, 
                      column_mapping: Dict[str, str],
                      chunk_index: int) -> List[TransactionRecord]:
        """å¤„ç†å•ä¸ªæ•°æ®å—"""
        records = []
        
        for idx, row in chunk.iterrows():
            try:
                record = self._convert_row_to_record(row, column_mapping)
                if record:
                    records.append(record)
            except Exception as e:
                self.logger.warning(f"å— {chunk_index} ç¬¬ {idx} è¡Œå¤„ç†å¤±è´¥: {e}")
        
        return records
    
    def _convert_row_to_record(self, 
                              row: pd.Series, 
                              column_mapping: Dict[str, str]) -> Optional[TransactionRecord]:
        """å°†Excelè¡Œè½¬æ¢ä¸ºäº¤æ˜“è®°å½•"""
        try:
            # ä½¿ç”¨ç¼“å­˜çš„åˆ—æ˜ å°„
            cache_key = str(sorted(column_mapping.items()))
            if cache_key not in self._column_mapping_cache:
                self._column_mapping_cache[cache_key] = column_mapping
            
            mapping = self._column_mapping_cache[cache_key]
            
            # æå–å¿…è¦å­—æ®µ
            date_str = str(row.get(mapping.get('date', ''), '')).strip()
            amount_str = str(row.get(mapping.get('amount', ''), '')).strip()
            description = str(row.get(mapping.get('description', ''), '')).strip()
            counterparty = str(row.get(mapping.get('counterparty', ''), '')).strip()
            
            # æ•°æ®éªŒè¯å’Œè½¬æ¢
            if not date_str or not amount_str or date_str == 'nan' or amount_str == 'nan':
                return None
            
            # æ—¥æœŸè½¬æ¢
            try:
                if isinstance(row.get(mapping.get('date', '')), pd.Timestamp):
                    transaction_date = row.get(mapping.get('date', '')).date()
                else:
                    transaction_date = pd.to_datetime(date_str).date()
            except:
                return None
            
            # é‡‘é¢è½¬æ¢
            try:
                amount = Decimal(str(amount_str).replace(',', '').replace('ï¿¥', '').replace('Â¥', ''))
                if amount == 0:
                    return None
            except:
                return None
            
            # äº¤æ˜“ç±»å‹åˆ¤æ–­
            transaction_type = TransactionType.INCOME if amount > 0 else TransactionType.EXPENSE
            amount = abs(amount)
            
            # åˆ›å»ºäº¤æ˜“è®°å½•
            record = TransactionRecord(
                id=f"import_{int(time.time() * 1000000)}_{hash(str(row.values))}",
                date=transaction_date,
                type=transaction_type,
                amount=amount,
                counterparty_id=counterparty or "æœªçŸ¥",
                description=description or "å¯¼å…¥äº¤æ˜“",
                category=self._auto_categorize(description),
                status=TransactionStatus.COMPLETED,
                created_at=datetime.now(),
                updated_at=datetime.now()
            )
            
            return record
            
        except Exception as e:
            self.logger.warning(f"è¡Œè½¬æ¢å¤±è´¥: {e}")
            return None
    
    def _auto_categorize(self, description: str) -> str:
        """è‡ªåŠ¨åˆ†ç±»äº¤æ˜“"""
        if not description:
            return "å…¶ä»–"
        
        description_lower = description.lower()
        
        # ç®€å•çš„å…³é”®è¯åŒ¹é…
        categories = {
            "é”€å”®": ["é”€å”®", "æ”¶å…¥", "è¥ä¸š", "æœåŠ¡è´¹"],
            "é‡‡è´­": ["é‡‡è´­", "è¿›è´§", "åŸææ–™", "ä¾›åº”å•†"],
            "è´¹ç”¨": ["è´¹ç”¨", "æ”¯å‡º", "åŠå…¬", "å·®æ—…", "ç§Ÿé‡‘"],
            "ç¨è´¹": ["ç¨", "å¢å€¼ç¨", "æ‰€å¾—ç¨", "å°èŠ±ç¨"],
            "é“¶è¡Œ": ["é“¶è¡Œ", "åˆ©æ¯", "æ‰‹ç»­è´¹", "è´·æ¬¾"]
        }
        
        for category, keywords in categories.items():
            if any(keyword in description_lower for keyword in keywords):
                return category
        
        return "å…¶ä»–"
    
    def _post_process_records(self, records: List[TransactionRecord]) -> List[TransactionRecord]:
        """åå¤„ç†ï¼šæ•°æ®æ¸…ç†å’ŒéªŒè¯"""
        cleaned_records = []
        
        for record in records:
            try:
                # æ•°æ®éªŒè¯
                if self._validate_record(record):
                    cleaned_records.append(record)
                else:
                    self.stats.error_rows += 1
            except Exception as e:
                self.logger.warning(f"è®°å½•éªŒè¯å¤±è´¥: {e}")
                self.stats.error_rows += 1
        
        # å»é‡
        unique_records = self._remove_duplicates(cleaned_records)
        
        return unique_records
    
    def _validate_record(self, record: TransactionRecord) -> bool:
        """éªŒè¯äº¤æ˜“è®°å½•"""
        # ä½¿ç”¨ç¼“å­˜æé«˜éªŒè¯æ€§èƒ½
        cache_key = f"{record.date}_{record.amount}_{record.description[:20]}"
        if cache_key in self._validation_cache:
            return self._validation_cache[cache_key]
        
        is_valid = (
            record.date is not None and
            record.amount > 0 and
            record.description.strip() != "" and
            len(record.description) <= 200
        )
        
        self._validation_cache[cache_key] = is_valid
        return is_valid
    
    def _remove_duplicates(self, records: List[TransactionRecord]) -> List[TransactionRecord]:
        """å»é™¤é‡å¤è®°å½•"""
        seen = set()
        unique_records = []
        
        for record in records:
            # åˆ›å»ºå”¯ä¸€æ ‡è¯†
            key = (record.date, record.amount, record.description, record.counterparty_id)
            if key not in seen:
                seen.add(key)
                unique_records.append(record)
        
        return unique_records
    
    def _get_memory_usage(self) -> float:
        """è·å–å½“å‰å†…å­˜ä½¿ç”¨é‡(MB)"""
        try:
            import psutil
            process = psutil.Process()
            return process.memory_info().rss / (1024 * 1024)
        except ImportError:
            return 0.0
    
    def get_performance_report(self) -> str:
        """è·å–æ€§èƒ½æŠ¥å‘Š"""
        return f"""
ğŸ“Š Excelå¤„ç†æ€§èƒ½æŠ¥å‘Š
==================
æ€»è¡Œæ•°: {self.stats.total_rows:,}
æˆåŠŸå¤„ç†: {self.stats.processed_rows:,}
é”™è¯¯è¡Œæ•°: {self.stats.error_rows:,}
æˆåŠŸç‡: {self.stats.success_rate:.1f}%
å¤„ç†æ—¶é—´: {self.stats.processing_time:.2f} ç§’
å¤„ç†é€Ÿåº¦: {self.stats.processed_rows / max(self.stats.processing_time, 0.001):.0f} è¡Œ/ç§’
å†…å­˜ä½¿ç”¨: {self.stats.memory_usage_mb:.1f} MB

âš¡ æ€§èƒ½ä¼˜åŒ–æ•ˆæœ:
- åˆ†å—å¤„ç†: æ”¯æŒä»»æ„å¤§å°æ–‡ä»¶
- å¤šçº¿ç¨‹: {self.max_workers} ä¸ªå·¥ä½œçº¿ç¨‹
- å†…å­˜ä¼˜åŒ–: é™åˆ¶ {self.memory_limit_mb} MB
- ç¼“å­˜æœºåˆ¶: æå‡é‡å¤æ“ä½œæ€§èƒ½
"""


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # åˆ›å»ºä¼˜åŒ–å¤„ç†å™¨
    processor = OptimizedExcelProcessor(
        chunk_size=2000,  # æ¯æ¬¡å¤„ç†2000è¡Œ
        max_workers=4,    # 4ä¸ªå·¥ä½œçº¿ç¨‹
        memory_limit_mb=300  # é™åˆ¶300MBå†…å­˜
    )
    
    # åˆ—æ˜ å°„é…ç½®
    column_mapping = {
        'date': 'æ—¥æœŸ',
        'amount': 'é‡‘é¢',
        'description': 'æ‘˜è¦',
        'counterparty': 'å¯¹æ–¹æˆ·å'
    }
    
    # å¤„ç†Excelæ–‡ä»¶
    file_path = Path("test_data.xlsx")
    progress = ProgressCallback(total_steps=1000)
    
    try:
        records, stats = processor.process_excel_file(
            file_path=file_path,
            column_mapping=column_mapping,
            progress_callback=progress
        )
        
        print(processor.get_performance_report())
        print(f"\nâœ… æˆåŠŸå¯¼å…¥ {len(records)} æ¡äº¤æ˜“è®°å½•")
        
    except Exception as e:
        print(f"âŒ å¤„ç†å¤±è´¥: {e}")